{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8849165,"sourceType":"datasetVersion","datasetId":5326487},{"sourceId":8865708,"sourceType":"datasetVersion","datasetId":5335975},{"sourceId":8866949,"sourceType":"datasetVersion","datasetId":5336516}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"Nespresso Text Multilabel David","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote, urlparse\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\nimport tarfile\nimport shutil\n\nCHUNK_SIZE = 40960\nDATA_SOURCE_MAPPING = 'trained-models:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4090276%2F7096809%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240702%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240702T070514Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D888c8b6a522e8742d54d84637123228f3a8a112bdd9713518d64a4e388a8e5479ab59ea5d173912cb65dfd605cefd49dc3d6cf6cbb037fe53bf64b1a08e7eac9abed23bc14dbe63b991a0051ad67df004ffa12eb681f9dec0caf9b9e729e63a82e90936601a6249718ad422f1f7d834f24b7607469fd4e5164f4e02899c9b6db77eee0d769f871144dd510f7c8204d93d1d11b085ff515edc18f3bc250bddcac69fe1ab1e345361a5d1175bf878865f6c830139ed028e064209423ec545562d2b747f9e8e8d2166121d60f5b9a1a94b19d4f01722d98c5b4cbabe01622c6bfd022bf1c8e057eca91bd2aeb529c2adfec28b30c8b69c99022651ec3df3726766c,data-train-150-topics:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4016962%2F7181985%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240702%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240702T070514Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D91e1e57f327719de470f039ae53d57f0f91fec7881d8ea16e45028f857b796c30e9bb69318afc23889ce6514c4042766d9b192ef524d3c23efb53d427ec1804efc426f0469ad9a07e98745fea4c1d976634912adb4830af8439d9325f3c66bb3afd54516672282d69dba626ad3405fae26880815337ad1cce1ce5ff6c59450c03e744fc0f8410cb6f5a21e003b193f9589333ed6e5c86fb857fcee45de157302e8df22f08dceb6b8633487b7991b0ef5cd487dc7e5d56afcdae0be158c3fc24132c0359e89ff7045d1d012549de4f2de441d9ccfc01efc3639d5c8030c1c6eec9b86b91cbd43b8fbf986dd71d407e48e9add5537ef4798b9fa1a0620225c93bd,data-train-all-categories:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4107047%2F7339028%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240702%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240702T070515Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0cdf55fccb337769e42891f9e8c94c0070c6de04ee182078419dd9a7d943727896f18fa44e6553072f46f796c6b7fcdb06d0e2d9f4b6e0e82ef5130455f7ffb63bdad17c21701ec05fd0847d19212faab513e899d6717333237bb4c6fabba52b0e847a877b2728dc79e5a2bc53fb0825c33df3c620b6e426cbc86e7e09e5fa66b930b30d822245f2021465aa85b2e6ee8a58526cca95f495e0beda50f14afcb5f0b592195002423c53452a24c009d669897e409f9ef8efb237613110567293b45e18c1d3dce833f3ad46dc6d4aec641507ea202e6bbcec8fd8e34f4219759eac6625ac829cedd0e71a181a4653b783ce6b6b89ff29fbe8e6921c321bbc5274f0'\n\nKAGGLE_INPUT_PATH='/kaggle/input'\nKAGGLE_WORKING_PATH='/kaggle/working'\nKAGGLE_SYMLINK='kaggle'\n\n!umount /kaggle/input/ 2> /dev/null\nshutil.rmtree('/kaggle/input', ignore_errors=True)\nos.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\nos.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n\ntry:\n  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\nexcept FileExistsError:\n  pass\ntry:\n  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\nexcept FileExistsError:\n  pass\n\nfor data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n    directory, download_url_encoded = data_source_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    filename = urlparse(download_url).path\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n            total_length = fileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes compressed')\n            dl = 0\n            data = fileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = fileres.read(CHUNK_SIZE)\n            if filename.endswith('.zip'):\n              with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n            else:\n              with tarfile.open(tfile.name) as tarfile:\n                tarfile.extractall(destination_path)\n            print(f'\\nDownloaded and uncompressed: {directory}')\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\n\nprint('Data source import complete.')\n","metadata":{"id":"pfDZxDZeQ0H0","execution":{"iopub.status.busy":"2024-07-05T06:22:35.630018Z","iopub.execute_input":"2024-07-05T06:22:35.630348Z","iopub.status.idle":"2024-07-05T06:23:11.880747Z","shell.execute_reply.started":"2024-07-05T06:22:35.630320Z","shell.execute_reply":"2024-07-05T06:23:11.879684Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading trained-models, 1075382447 bytes compressed\n[==================================================] 1075382447 bytes downloadedFailed to load https://storage.googleapis.com/kaggle-data-sets/4090276/7096809/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240702%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240702T070514Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=888c8b6a522e8742d54d84637123228f3a8a112bdd9713518d64a4e388a8e5479ab59ea5d173912cb65dfd605cefd49dc3d6cf6cbb037fe53bf64b1a08e7eac9abed23bc14dbe63b991a0051ad67df004ffa12eb681f9dec0caf9b9e729e63a82e90936601a6249718ad422f1f7d834f24b7607469fd4e5164f4e02899c9b6db77eee0d769f871144dd510f7c8204d93d1d11b085ff515edc18f3bc250bddcac69fe1ab1e345361a5d1175bf878865f6c830139ed028e064209423ec545562d2b747f9e8e8d2166121d60f5b9a1a94b19d4f01722d98c5b4cbabe01622c6bfd022bf1c8e057eca91bd2aeb529c2adfec28b30c8b69c99022651ec3df3726766c to path /kaggle/input/trained-models\nDownloading data-train-150-topics, 3404880 bytes compressed\n[==================================================] 3404880 bytes downloadedFailed to load https://storage.googleapis.com/kaggle-data-sets/4016962/7181985/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240702%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240702T070514Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=91e1e57f327719de470f039ae53d57f0f91fec7881d8ea16e45028f857b796c30e9bb69318afc23889ce6514c4042766d9b192ef524d3c23efb53d427ec1804efc426f0469ad9a07e98745fea4c1d976634912adb4830af8439d9325f3c66bb3afd54516672282d69dba626ad3405fae26880815337ad1cce1ce5ff6c59450c03e744fc0f8410cb6f5a21e003b193f9589333ed6e5c86fb857fcee45de157302e8df22f08dceb6b8633487b7991b0ef5cd487dc7e5d56afcdae0be158c3fc24132c0359e89ff7045d1d012549de4f2de441d9ccfc01efc3639d5c8030c1c6eec9b86b91cbd43b8fbf986dd71d407e48e9add5537ef4798b9fa1a0620225c93bd to path /kaggle/input/data-train-150-topics\nDownloading data-train-all-categories, 4979274 bytes compressed\n[==================================================] 4979274 bytes downloadedFailed to load https://storage.googleapis.com/kaggle-data-sets/4107047/7339028/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240702%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240702T070515Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0cdf55fccb337769e42891f9e8c94c0070c6de04ee182078419dd9a7d943727896f18fa44e6553072f46f796c6b7fcdb06d0e2d9f4b6e0e82ef5130455f7ffb63bdad17c21701ec05fd0847d19212faab513e899d6717333237bb4c6fabba52b0e847a877b2728dc79e5a2bc53fb0825c33df3c620b6e426cbc86e7e09e5fa66b930b30d822245f2021465aa85b2e6ee8a58526cca95f495e0beda50f14afcb5f0b592195002423c53452a24c009d669897e409f9ef8efb237613110567293b45e18c1d3dce833f3ad46dc6d4aec641507ea202e6bbcec8fd8e34f4219759eac6625ac829cedd0e71a181a4653b783ce6b6b89ff29fbe8e6921c321bbc5274f0 to path /kaggle/input/data-train-all-categories\nData source import complete.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT Beto\n\nImportación de las librerías y carga del modelo beto, el aviso:\n> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference\n\nEs un aviso normal, lo importante es que el modelo se ha cargado correctamente y por tanto esta preparado para que pueda ser entrenado, el aviso viene dado por la última capa de salida del modelo el 'classifier' esta es la capa de debe personalizarse para nuestro objetivo, y por tanto, al no encontrar ninguna ha cargado una aleatoria\n\n\n* **ignore_mismatched_sizes=True**:Este argumento se utiliza para ignorar las diferencias en el tamaño de las matrices de peso preentrenadas y las matrices de peso del modelo que estás cargando.Se utiliza cuando quieres adaptar un modelo preentrenado a una tarea de clasificación que tiene un número de clases diferente al modelo preentrenado. El modelo preentrenado puede tener un número diferente de clases, y al establecer ignore_mismatched_sizes=True, le indicas a PyTorch que ignore la diferencia en el número de clases y utilice las capas que son compatibles, lo que puede ser útil para la afinación de modelos preentrenados en tareas de clasificación personalizadas\n","metadata":{"id":"2CHsqYZvQ0H5"}},{"cell_type":"code","source":"model_name=\"/kaggle/input/modelo-bert/bert_base\"\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True, problem_type=\"multi_label_classification\")\nnum_total_layers = model.config.num_hidden_layers\nprint(num_total_layers)\nfor name, param in model.named_parameters():\n    print(name)","metadata":{"id":"YFz6dd5tQ0H7","execution":{"iopub.status.busy":"2024-07-05T06:24:16.135729Z","iopub.execute_input":"2024-07-05T06:24:16.136270Z","iopub.status.idle":"2024-07-05T06:24:20.852287Z","shell.execute_reply.started":"2024-07-05T06:24:16.136239Z","shell.execute_reply":"2024-07-05T06:24:20.851280Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/modelo-bert/bert_base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"12\nbert.embeddings.word_embeddings.weight\nbert.embeddings.position_embeddings.weight\nbert.embeddings.token_type_embeddings.weight\nbert.embeddings.LayerNorm.weight\nbert.embeddings.LayerNorm.bias\nbert.encoder.layer.0.attention.self.query.weight\nbert.encoder.layer.0.attention.self.query.bias\nbert.encoder.layer.0.attention.self.key.weight\nbert.encoder.layer.0.attention.self.key.bias\nbert.encoder.layer.0.attention.self.value.weight\nbert.encoder.layer.0.attention.self.value.bias\nbert.encoder.layer.0.attention.output.dense.weight\nbert.encoder.layer.0.attention.output.dense.bias\nbert.encoder.layer.0.attention.output.LayerNorm.weight\nbert.encoder.layer.0.attention.output.LayerNorm.bias\nbert.encoder.layer.0.intermediate.dense.weight\nbert.encoder.layer.0.intermediate.dense.bias\nbert.encoder.layer.0.output.dense.weight\nbert.encoder.layer.0.output.dense.bias\nbert.encoder.layer.0.output.LayerNorm.weight\nbert.encoder.layer.0.output.LayerNorm.bias\nbert.encoder.layer.1.attention.self.query.weight\nbert.encoder.layer.1.attention.self.query.bias\nbert.encoder.layer.1.attention.self.key.weight\nbert.encoder.layer.1.attention.self.key.bias\nbert.encoder.layer.1.attention.self.value.weight\nbert.encoder.layer.1.attention.self.value.bias\nbert.encoder.layer.1.attention.output.dense.weight\nbert.encoder.layer.1.attention.output.dense.bias\nbert.encoder.layer.1.attention.output.LayerNorm.weight\nbert.encoder.layer.1.attention.output.LayerNorm.bias\nbert.encoder.layer.1.intermediate.dense.weight\nbert.encoder.layer.1.intermediate.dense.bias\nbert.encoder.layer.1.output.dense.weight\nbert.encoder.layer.1.output.dense.bias\nbert.encoder.layer.1.output.LayerNorm.weight\nbert.encoder.layer.1.output.LayerNorm.bias\nbert.encoder.layer.2.attention.self.query.weight\nbert.encoder.layer.2.attention.self.query.bias\nbert.encoder.layer.2.attention.self.key.weight\nbert.encoder.layer.2.attention.self.key.bias\nbert.encoder.layer.2.attention.self.value.weight\nbert.encoder.layer.2.attention.self.value.bias\nbert.encoder.layer.2.attention.output.dense.weight\nbert.encoder.layer.2.attention.output.dense.bias\nbert.encoder.layer.2.attention.output.LayerNorm.weight\nbert.encoder.layer.2.attention.output.LayerNorm.bias\nbert.encoder.layer.2.intermediate.dense.weight\nbert.encoder.layer.2.intermediate.dense.bias\nbert.encoder.layer.2.output.dense.weight\nbert.encoder.layer.2.output.dense.bias\nbert.encoder.layer.2.output.LayerNorm.weight\nbert.encoder.layer.2.output.LayerNorm.bias\nbert.encoder.layer.3.attention.self.query.weight\nbert.encoder.layer.3.attention.self.query.bias\nbert.encoder.layer.3.attention.self.key.weight\nbert.encoder.layer.3.attention.self.key.bias\nbert.encoder.layer.3.attention.self.value.weight\nbert.encoder.layer.3.attention.self.value.bias\nbert.encoder.layer.3.attention.output.dense.weight\nbert.encoder.layer.3.attention.output.dense.bias\nbert.encoder.layer.3.attention.output.LayerNorm.weight\nbert.encoder.layer.3.attention.output.LayerNorm.bias\nbert.encoder.layer.3.intermediate.dense.weight\nbert.encoder.layer.3.intermediate.dense.bias\nbert.encoder.layer.3.output.dense.weight\nbert.encoder.layer.3.output.dense.bias\nbert.encoder.layer.3.output.LayerNorm.weight\nbert.encoder.layer.3.output.LayerNorm.bias\nbert.encoder.layer.4.attention.self.query.weight\nbert.encoder.layer.4.attention.self.query.bias\nbert.encoder.layer.4.attention.self.key.weight\nbert.encoder.layer.4.attention.self.key.bias\nbert.encoder.layer.4.attention.self.value.weight\nbert.encoder.layer.4.attention.self.value.bias\nbert.encoder.layer.4.attention.output.dense.weight\nbert.encoder.layer.4.attention.output.dense.bias\nbert.encoder.layer.4.attention.output.LayerNorm.weight\nbert.encoder.layer.4.attention.output.LayerNorm.bias\nbert.encoder.layer.4.intermediate.dense.weight\nbert.encoder.layer.4.intermediate.dense.bias\nbert.encoder.layer.4.output.dense.weight\nbert.encoder.layer.4.output.dense.bias\nbert.encoder.layer.4.output.LayerNorm.weight\nbert.encoder.layer.4.output.LayerNorm.bias\nbert.encoder.layer.5.attention.self.query.weight\nbert.encoder.layer.5.attention.self.query.bias\nbert.encoder.layer.5.attention.self.key.weight\nbert.encoder.layer.5.attention.self.key.bias\nbert.encoder.layer.5.attention.self.value.weight\nbert.encoder.layer.5.attention.self.value.bias\nbert.encoder.layer.5.attention.output.dense.weight\nbert.encoder.layer.5.attention.output.dense.bias\nbert.encoder.layer.5.attention.output.LayerNorm.weight\nbert.encoder.layer.5.attention.output.LayerNorm.bias\nbert.encoder.layer.5.intermediate.dense.weight\nbert.encoder.layer.5.intermediate.dense.bias\nbert.encoder.layer.5.output.dense.weight\nbert.encoder.layer.5.output.dense.bias\nbert.encoder.layer.5.output.LayerNorm.weight\nbert.encoder.layer.5.output.LayerNorm.bias\nbert.encoder.layer.6.attention.self.query.weight\nbert.encoder.layer.6.attention.self.query.bias\nbert.encoder.layer.6.attention.self.key.weight\nbert.encoder.layer.6.attention.self.key.bias\nbert.encoder.layer.6.attention.self.value.weight\nbert.encoder.layer.6.attention.self.value.bias\nbert.encoder.layer.6.attention.output.dense.weight\nbert.encoder.layer.6.attention.output.dense.bias\nbert.encoder.layer.6.attention.output.LayerNorm.weight\nbert.encoder.layer.6.attention.output.LayerNorm.bias\nbert.encoder.layer.6.intermediate.dense.weight\nbert.encoder.layer.6.intermediate.dense.bias\nbert.encoder.layer.6.output.dense.weight\nbert.encoder.layer.6.output.dense.bias\nbert.encoder.layer.6.output.LayerNorm.weight\nbert.encoder.layer.6.output.LayerNorm.bias\nbert.encoder.layer.7.attention.self.query.weight\nbert.encoder.layer.7.attention.self.query.bias\nbert.encoder.layer.7.attention.self.key.weight\nbert.encoder.layer.7.attention.self.key.bias\nbert.encoder.layer.7.attention.self.value.weight\nbert.encoder.layer.7.attention.self.value.bias\nbert.encoder.layer.7.attention.output.dense.weight\nbert.encoder.layer.7.attention.output.dense.bias\nbert.encoder.layer.7.attention.output.LayerNorm.weight\nbert.encoder.layer.7.attention.output.LayerNorm.bias\nbert.encoder.layer.7.intermediate.dense.weight\nbert.encoder.layer.7.intermediate.dense.bias\nbert.encoder.layer.7.output.dense.weight\nbert.encoder.layer.7.output.dense.bias\nbert.encoder.layer.7.output.LayerNorm.weight\nbert.encoder.layer.7.output.LayerNorm.bias\nbert.encoder.layer.8.attention.self.query.weight\nbert.encoder.layer.8.attention.self.query.bias\nbert.encoder.layer.8.attention.self.key.weight\nbert.encoder.layer.8.attention.self.key.bias\nbert.encoder.layer.8.attention.self.value.weight\nbert.encoder.layer.8.attention.self.value.bias\nbert.encoder.layer.8.attention.output.dense.weight\nbert.encoder.layer.8.attention.output.dense.bias\nbert.encoder.layer.8.attention.output.LayerNorm.weight\nbert.encoder.layer.8.attention.output.LayerNorm.bias\nbert.encoder.layer.8.intermediate.dense.weight\nbert.encoder.layer.8.intermediate.dense.bias\nbert.encoder.layer.8.output.dense.weight\nbert.encoder.layer.8.output.dense.bias\nbert.encoder.layer.8.output.LayerNorm.weight\nbert.encoder.layer.8.output.LayerNorm.bias\nbert.encoder.layer.9.attention.self.query.weight\nbert.encoder.layer.9.attention.self.query.bias\nbert.encoder.layer.9.attention.self.key.weight\nbert.encoder.layer.9.attention.self.key.bias\nbert.encoder.layer.9.attention.self.value.weight\nbert.encoder.layer.9.attention.self.value.bias\nbert.encoder.layer.9.attention.output.dense.weight\nbert.encoder.layer.9.attention.output.dense.bias\nbert.encoder.layer.9.attention.output.LayerNorm.weight\nbert.encoder.layer.9.attention.output.LayerNorm.bias\nbert.encoder.layer.9.intermediate.dense.weight\nbert.encoder.layer.9.intermediate.dense.bias\nbert.encoder.layer.9.output.dense.weight\nbert.encoder.layer.9.output.dense.bias\nbert.encoder.layer.9.output.LayerNorm.weight\nbert.encoder.layer.9.output.LayerNorm.bias\nbert.encoder.layer.10.attention.self.query.weight\nbert.encoder.layer.10.attention.self.query.bias\nbert.encoder.layer.10.attention.self.key.weight\nbert.encoder.layer.10.attention.self.key.bias\nbert.encoder.layer.10.attention.self.value.weight\nbert.encoder.layer.10.attention.self.value.bias\nbert.encoder.layer.10.attention.output.dense.weight\nbert.encoder.layer.10.attention.output.dense.bias\nbert.encoder.layer.10.attention.output.LayerNorm.weight\nbert.encoder.layer.10.attention.output.LayerNorm.bias\nbert.encoder.layer.10.intermediate.dense.weight\nbert.encoder.layer.10.intermediate.dense.bias\nbert.encoder.layer.10.output.dense.weight\nbert.encoder.layer.10.output.dense.bias\nbert.encoder.layer.10.output.LayerNorm.weight\nbert.encoder.layer.10.output.LayerNorm.bias\nbert.encoder.layer.11.attention.self.query.weight\nbert.encoder.layer.11.attention.self.query.bias\nbert.encoder.layer.11.attention.self.key.weight\nbert.encoder.layer.11.attention.self.key.bias\nbert.encoder.layer.11.attention.self.value.weight\nbert.encoder.layer.11.attention.self.value.bias\nbert.encoder.layer.11.attention.output.dense.weight\nbert.encoder.layer.11.attention.output.dense.bias\nbert.encoder.layer.11.attention.output.LayerNorm.weight\nbert.encoder.layer.11.attention.output.LayerNorm.bias\nbert.encoder.layer.11.intermediate.dense.weight\nbert.encoder.layer.11.intermediate.dense.bias\nbert.encoder.layer.11.output.dense.weight\nbert.encoder.layer.11.output.dense.bias\nbert.encoder.layer.11.output.LayerNorm.weight\nbert.encoder.layer.11.output.LayerNorm.bias\nbert.pooler.dense.weight\nbert.pooler.dense.bias\nclassifier.weight\nclassifier.bias\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\nfrom typing import Optional\n\nclass BertModel():\n    \"\"\"\n        Clase modelo BERT\n    \"\"\"\n\n    model:BertForSequenceClassification = None\n    tokenizer:BertTokenizer = None\n\n    #dccuchile/bert-base-spanish-wwm-cased\n    #xlm-roberta-base\n    def __init__(self, num_labels:int, model_name: Optional[str] = \"xlm-roberta-base\"):\n        super().__init__()\n\n        if num_labels is None or num_labels == 0:\n            raise ValueError(\"Para cargar el módelo BERT es necesario indicar el número de clases que debe códificar\")\n\n        try:\n            #self.tokenizer = BertTokenizer.from_pretrained(model_name)\n            #self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True, problem_type=\"multi_label_classification\")\n\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True, problem_type=\"multi_label_classification\")\n\n            # Congelar todas las capas excepto las últimas N\n            #num_total_layers = self.model.config.num_hidden_layers\n            #for name, param in self.model.named_parameters():\n            #    if 'layer' in name:\n            #        layer_num = int(name.split('.')[3])\n            #        if layer_num < num_total_layers - 5:\n            #            param.requires_grad = False\n\n        except Exception as e:\n            print(f\"Se produjo un error al cargar el modelo: {e}\")\n            self.model = None\n        pass\n\n    def configure_optimizers(self):\n        # Inicializa tu optimizador con un lr y weight_decay inicial\n        optimizer = torch.optim.AdamW(self.parameters(), lr=2e-5, weight_decay=0.01)\n\n        # Define un programador de la tasa de aprendizaje (lr_scheduler)\n        # Por ejemplo, ReduceLROnPlateau para ajustar el lr basado en la validación de la pérdida\n        lr_scheduler = {\n            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3),\n            'monitor': 'val_loss',\n        }\n\n        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}","metadata":{"scrolled":true,"id":"I3Qklh8SQ0H7","execution":{"iopub.status.busy":"2024-07-05T06:24:08.247384Z","iopub.execute_input":"2024-07-05T06:24:08.247746Z","iopub.status.idle":"2024-07-05T06:24:13.889474Z","shell.execute_reply.started":"2024-07-05T06:24:08.247716Z","shell.execute_reply":"2024-07-05T06:24:13.888585Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Carga y preparación datasets entrenamiento\n\n1. Carga de datos\n2. Preparación de los datos para que los entienda el modelo BERT\n3. División de los datos cargados para entrenar el modelo vs validación posterior\n\n\n**Carga de datos**\n\nloadDatasetCsv: Es la función encargada de leer lo datos del fichero CSV pasado como parametro y devolver 2 vectores\n* texts: Contiene los textos de la columna TEXTS\n* lablels: Contiene lo topics IDS\n\n**Preparación de los datos para que los entienda el modelo BERT**\n\nEn este frágmento de código, se cargan los datos del fichero que se quiera utilizar para el entrenamiento, se convierten a al formato que BERT pueda entender\n\n1. **Tokenización:** Divide el texto en tokens (palabras o subpalabras), los cuales son las unidades básicas que el modelo puede procesar.\n2. **Conversión a IDs:** Convierte los tokens en sus respectivos IDs de acuerdo con el vocabulario del modelo BERT. Cada token tiene un ID único que lo representa en el vocabulario.\n3. **Padding:** Iguala la longitud de todas las secuencias de tokens a una longitud máxima especificada. Si el padding está configurado a \"max_length\", todas las secuencias se rellenarán hasta la max_length dada, en este caso, 512 tokens. Esto se hace porque BERT requiere que todas las secuencias de entrada tengan la misma longitud para el procesamiento por lotes.\n4. **Truncamiento:** Si el texto es más largo que la longitud máxima permitida (max_length), se cortará a esa longitud para asegurar que no exceda el número máximo de tokens que el modelo puede manejar. Para BERT, la longitud máxima estándar es 512 tokens.\n5. **Retorno de Tensores:** El parámetro return_tensors=\"pt\" indica que los datos procesados deben ser convertidos a tensores de PyTorch, que es el tipo de dato requerido para alimentar al modelo BERT durante el entrenamiento o la evaluación.\n\nLa salida de esta línea de código será un diccionario que contiene los siguientes elementos, listos para ser utilizados como entrada del modelo BERT:\n\n1. **input_ids:** Un tensor que contiene los IDs de los tokens.\n2. **attention_mask:** Un tensor que indica a BERT qué tokens deben ser atendidos y cuáles son relleno (padding). Los tokens reales tienen un valor de 1, y los tokens de relleno tienen un valor de 0.\n3. **token_type_ids:** (Si el tokenizer los soporta y es relevante para el modelo) Un tensor que puede diferenciar entre diferentes secuencias dentro de la misma entrada, útil para tareas que involucran múltiples secuencias como preguntas y respuestas o comparaciones de texto\n\n**División de los datos cargados para entrenar el modelo vs validación posterior**\n\nDividir el conjunto de datos en dos SETS, uno para entrenar y otro para validar posteriormente el modelo entrenado\n\n* **test_size**=0.2: Especifica que el 20% de los datos se reservará para el conjunto de validación, mientras que el 80% restante se usará para el entrenamiento.\n* **random_state**=42: Es una semilla para el generador de números aleatorios que se utiliza para dividir los datos. Usar un número fijo aquí significa que el split será reproducible; es decir, obtendrás la misma división cada vez que ejecutes este código.","metadata":{"id":"waiexVWoQ0H8"}},{"cell_type":"code","source":"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom typing import Optional,Dict\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom deprecated import deprecated\n\nclass DataLoaderBert:\n\n    path:str = None\n    df:DataFrame = None\n    num_labels:int = 0\n    column_label:str = None\n    labels=None\n\n    def __init__(self, path:str, column_label:str, dtype: Optional[Dict[str, str]]= None):\n        self.path = path\n        self.column_label = column_label\n        self._loadDatasetCsv(dtype=dtype)\n        pass\n\n    def _loadDatasetCsv(self, dtype: Optional[Dict[str, str]]= None):\n        \"\"\"\n        Carga los datos desde un archivo CSV y devuelve las listas de textos y etiquetas.\n\n        Args:\n            path (str): La ruta al archivo CSV que contiene los datos.\n            dtype(Dict[str, str]): La clave del diccionario se corresponde con el campo del csv mientrás que el value del diccionario con el tipo de dato del campo\n            plot_field (str): si queremos mostrar un grafico para ver lo equilibrado que esta el ds por un campo determinado\n\n        Returns:\n            data (DataFrame): DataFrame pandas.\n        \"\"\"\n\n        data = pd.read_csv(self.path, dtype=dtype)\n\n        if not \"VERBATIM\" in data:\n            raise ValueError(\"El dataset que se quiere cargar no dispone del campo VERBATIM, este es obligatorio\")\n\n        data[\"VERBATIM\"] = data[\"VERBATIM\"].astype(\"str\")\n        data[self.column_label] = data[self.column_label].str.split(',')\n\n        data.dropna()\n        #data.drop_duplicates()\n        data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n        self.df = data;\n        self.labels = data.explode(self.column_label)[self.column_label].unique()\n        self.num_labels = self.labels.size\n\n    def explain(self):\n        sns.countplot(x = self.column_label, data = self.df)\n        plt.xlabel(self.column_label)\n        plt.show()\n        print(self.df.info())\n\n    def explain2(self):\n        # Supongamos que self.column_label contiene una lista de elementos\n        # Combina todas las etiquetas en un solo registro como una cadena\n        self.df['combined_labels'] = self.df[self.column_label].apply(lambda x: ', '.join(x))\n\n        # Cree un nuevo DataFrame con las etiquetas combinadas\n        combined_df = pd.DataFrame({'combined_labels': self.df['combined_labels']})\n\n        # Utilice la función melt en el nuevo DataFrame\n        melted_df = pd.melt(combined_df, value_vars=['combined_labels'], value_name='combined_labels')\n\n        # Divida las etiquetas combinadas en una lista de etiquetas separadas\n        melted_df['combined_labels'] = melted_df['combined_labels'].str.split(', ')\n\n        # Cree countplots para cada categoría en las etiquetas combinadas\n        plt.figure(figsize=(10, 6))  # Tamaño de la figura\n        sns.countplot(data=melted_df, x='combined_labels', order=melted_df['combined_labels'].value_counts().index)\n        plt.title('Countplots para etiquetas combinadas')\n        plt.xlabel('Etiquetas Combinadas')\n        plt.ylabel('Count')\n        plt.xticks(rotation=90)  # Rotar las etiquetas en el eje x para mayor claridad\n        plt.show()\n","metadata":{"id":"NixL-VnoQ0H9","execution":{"iopub.status.busy":"2024-07-05T06:24:25.367070Z","iopub.execute_input":"2024-07-05T06:24:25.367407Z","iopub.status.idle":"2024-07-05T06:24:26.490380Z","shell.execute_reply.started":"2024-07-05T06:24:25.367381Z","shell.execute_reply":"2024-07-05T06:24:26.489514Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/dataset-borrados/ds-subtopics-combined-v12-valores_borrados.csv')\nprint(df['CODES'].unique())","metadata":{"id":"oK9GQkRLQ0H9","execution":{"iopub.status.busy":"2024-07-05T06:24:29.784351Z","iopub.execute_input":"2024-07-05T06:24:29.785186Z","iopub.status.idle":"2024-07-05T06:24:29.904170Z","shell.execute_reply.started":"2024-07-05T06:24:29.785152Z","shell.execute_reply":"2024-07-05T06:24:29.903182Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['103' '104' '205' ... '0808,0104,0501' '9201,9701,9801' '0204,0104,0102']\n","output_type":"stream"}]},{"cell_type":"code","source":"dtype:Dict[str,str]={\"VERBATIM\":\"str\",\"CODES\":\"str\"}\ndl_test:DataLoaderBert = DataLoaderBert(\"/kaggle/input/dataset-borrados/ds-subtopics-combined-v12-valores_borrados.csv\",\"CODES\",dtype)\nprint(dl_test.labels)","metadata":{"id":"VRN4CcPaQ0H-","execution":{"iopub.status.busy":"2024-07-05T06:24:31.310172Z","iopub.execute_input":"2024-07-05T06:24:31.310859Z","iopub.status.idle":"2024-07-05T06:24:31.433660Z","shell.execute_reply.started":"2024-07-05T06:24:31.310819Z","shell.execute_reply":"2024-07-05T06:24:31.432733Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['207' '708' '0405' '101' '9203' '0301' '9202' '0702' '406' '9501' '205'\n '0404' '103' '0701' '302' '9301' '0403' '9201' '405' '0501' '701' '203'\n '9401' '301' '0302' '9101' '808' '104' '206' '9801' '9702' '0104' '9601'\n '403' '0207' '201' '9701' '702' '404' '204' '0103' '0201' '202' '501'\n '0102' '0205' '402' '0204' '401' '102' '0208' '0101' '0202' '0401' '0808'\n '0402' '601' '208' '0601' '0203' '0708' '0206' '0406']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tuning desde cero\n\nEl \"fine-tuning\" de un modelo en el contexto del aprendizaje automático se refiere al proceso de ajustar un modelo que ha sido previamente entrenado en un conjunto de datos grande y general (pre-entrenamiento) para que se adapte mejor a un conjunto de datos específico o a una tarea concreta. A veces se le llama \"afinado\" o \"ajuste fino\" en español, manteniendo la idea de realizar ajustes minuciosos para mejorar la performance del modelo en una tarea específica.\n\nEn nuestro caso particular tomamos el modelo BERT (BETO)","metadata":{"id":"3X60LV5BQ0H-"}},{"cell_type":"markdown","source":"**TrainerUtils**\nSe trata de una clase de utilidades que nos permite y nos ayuda a traves de sus métodos, analizar los resultados guardados en el output_dir del trainer\n* **printMetrics:** Nos permite recorrecorer todos los checkpoint y imprimir su métrica de perdida de entrenamiento y validación\n* **searchBestCheckpoint:** Busca que checkpoint tiene mejor métrica  de perdida\n* **removeWorstCheckpoint:** Libera espacio, eliminando aquellos checkpoint que tienen peores métricas\n* **loadBestModel:** Carga el modelo con mejores métricas a partir de los checkpoints\n* **loadTokenizer:** Carga el tokenizer con mejores métricas a partir de los checkpoints","metadata":{"id":"6ldRs8-rQ0H_"}},{"cell_type":"markdown","source":"**Funciones para imprimir las métricas F1, roc_auc y accuracy durante el entrenamiento**","metadata":{"id":"NK2_pii5Q0H_"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score,roc_auc_score,accuracy_score\nfrom transformers import EvalPrediction\nimport numpy as np\nimport torch\n\n# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n    # next, use threshold to turn them into integer predictions\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n    # finally, compute metrics\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro') #usamos micro debido al multietiquetado\n    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro') #usamos micro debido al multietiquetado\n    accuracy = accuracy_score(y_true, y_pred)\n    # return as dictionary\n    metrics = {'f1': f1_micro_average,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy}\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    print(p)\n    preds = p.predictions[0] if isinstance(p.predictions,\n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds,\n        labels=p.label_ids)\n    return result","metadata":{"id":"Ef4PYFKjQ0H_","execution":{"iopub.status.busy":"2024-07-05T06:24:34.822241Z","iopub.execute_input":"2024-07-05T06:24:34.822933Z","iopub.status.idle":"2024-07-05T06:24:34.835531Z","shell.execute_reply.started":"2024-07-05T06:24:34.822899Z","shell.execute_reply":"2024-07-05T06:24:34.834697Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Clase TrainerUtils tiene funciones de utilidad para gestionar las salidas del entrenamiento, eliminar checkpoints, guardar modelos, pintar métricas, etc**","metadata":{"id":"D2DjOCqoQ0IA"}},{"cell_type":"code","source":"import os\nimport shutil\nimport json\nfrom transformers import BertConfig, BertForSequenceClassification, BertTokenizer, AutoModelForSequenceClassification, AutoTokenizer, Trainer,DataCollatorWithPadding\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom datasets import Dataset\nimport pandas as pd\n\nclass TrainerUtils:\n\n    output_dir:str = None\n    best_metric:float = float('inf')\n    best_checkpoint:str = None\n\n    def __init__(self,output_dir:str):\n        self.output_dir = output_dir\n\n    def printMetrics(self):\n\n        # Recorrer todos los subdirectorios en el directorio de salida\n        for subdir in os.listdir(self.output_dir):\n            checkpoint_dir = os.path.join(self.output_dir, subdir)\n            if os.path.isdir(checkpoint_dir) and subdir.startswith(\"checkpoint\"):\n                # Leer el trainer_state.json de cada checkpoint\n                with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'r') as f:\n                    trainer_state = json.load(f)\n                # Comparar la métrica de evaluación y actualizar la mejor según sea necesario\n                eval_loss = trainer_state['log_history'][-1]['eval_loss']\n                loss = trainer_state['log_history'][-2]['loss']\n                print(f\"metrics {checkpoint_dir}, loss= {loss} eval_loss= {eval_loss}\")\n\n\n    def searchBestCheckpoint(self) -> str:\n\n        # Recorrer todos los subdirectorios en el directorio de salida\n        for subdir in os.listdir(self.output_dir):\n            checkpoint_dir = os.path.join(self.output_dir, subdir)\n            if os.path.isdir(checkpoint_dir) and subdir.startswith(\"checkpoint\"):\n                # Leer el trainer_state.json de cada checkpoint\n                try:\n                    with open(os.path.join(checkpoint_dir, 'trainer_state.json'), 'r') as f:\n                        trainer_state = json.load(f)\n                    # Comparar la métrica de evaluación y actualizar la mejor según sea necesario\n                    eval_loss = trainer_state['log_history'][-1]['eval_loss']\n                    if eval_loss < self.best_metric:\n                        self.best_metric = eval_loss\n                        self.best_checkpoint = checkpoint_dir\n                except FileNotFoundError:\n                    print(f\"No se encontró trainer_state.json en {checkpoint_dir}\")\n\n        return self.best_checkpoint\n\n    def saveBestCheckpoint(self,new_dir:Optional[str]=\"/kaggle/working/backup-best-checkpoint\"):\n        shutil.copytree(self.searchBestCheckpoint(), new_dir, dirs_exist_ok=True)\n\n    def removeWorstCheckpoint(self):\n\n        self.searchBestCheckpoint()\n        # Borrar todos los checkpoints excepto el mejor\n        for subdir in os.listdir(self.output_dir):\n            checkpoint_dir = os.path.join(self.output_dir, subdir)\n            if self.best_checkpoint==None:\n                return\n\n            if checkpoint_dir != self.best_checkpoint and os.path.isdir(checkpoint_dir) and subdir.startswith(\"checkpoint\"):\n                shutil.rmtree(checkpoint_dir)\n\n    def removeDirFolders(self):\n\n        for subdir in os.listdir(self.output_dir):\n            checkpoint_dir = os.path.join(self.output_dir, subdir)\n            if os.path.isdir(checkpoint_dir):\n                shutil.rmtree(checkpoint_dir)\n        print('Elementos eliminados')\n\n    def loadBestModel(self)->AutoModelForSequenceClassification:\n        return AutoModelForSequenceClassification.from_pretrained(self.searchBestCheckpoint())\n\n    def loadBestTokenizer(self)->AutoTokenizer:\n        return AutoTokenizer.from_pretrained(self.searchBestCheckpoint())\n\n    def compressModel(self, name:str, path:str):\n        archivo_zip = shutil.make_archive(name, \"zip\", path)\n\n    def compressBestModel(self):\n        best_model = self.searchBestCheckpoint()\n        name = best_model.split('/')[-1]\n        self.compressModel(name, best_model)\n\n    def predictDataset(self, model_name:str, data_loader:DataLoader, threshold: float = 0.5):\n\n        #El dataset usado para dloader tiene que ser el mismo que el de entrenamiento, al menos contener las mismas etiquetas.\n        dloader:DataLoaderBert = DataLoaderBert(\"/kaggle/input/dataset-borrados/ds-subtopics-combined-v12-valores_borrados.csv\",\"CODES\",{\"VERBATIM\":\"str\",\"CODES\":\"str\"})\n        multiLabelBinarizer:MultiLabelBinarizer = MultiLabelBinarizer(classes=dloader.labels)\n        data_loader.df['labels'] = list(multiLabelBinarizer.fit_transform(data_loader.df['CODES']).astype(float))\n        dataset:Dataset = Dataset.from_pandas(data_loader.df)\n\n        # Configuración personalizada del modelo\n        config = BertConfig.from_pretrained(model_name)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model =  AutoModelForSequenceClassification.from_pretrained(model_name,config=config)\n\n        def tokenize_function(examples):\n            tokenized_inputs = tokenizer(examples[\"VERBATIM\"], padding=\"max_length\", truncation=True, max_length=512)\n            tokenized_inputs[\"labels\"] = [list(map(float, label)) for label in examples[\"labels\"]]\n            return tokenized_inputs\n\n        tokenized_datasets = dataset.map(tokenize_function, batched=True)\n        data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')\n        trainer = Trainer(\n            model=model,\n            eval_dataset=tokenized_datasets,\n            tokenizer=tokenizer,\n            data_collator=data_collator\n        )\n\n        raw_pred, labels, _ = trainer.predict(tokenized_datasets)\n\n\n        # Asegurarse de que raw_pred es un tensor de PyTorch\n        if isinstance(raw_pred, np.ndarray):\n            raw_pred = torch.from_numpy(raw_pred)\n        sigmoid = torch.nn.Sigmoid()\n        probs = sigmoid(raw_pred)\n        predictions = np.zeros(probs.shape)\n\n        predictions[np.where(probs >= threshold)] = 1\n\n        predicted_labels_multilabel = multiLabelBinarizer.inverse_transform(predictions)\n        # Mostrar las probabilidades multilabel\n\n\n        dataset = dataset.to_pandas()\n        res_dataset = dataset.drop(columns = 'labels')\n        print(type(dataset))\n        res_dataset['predicted'] = predicted_labels_multilabel\n        print(res_dataset['predicted'][0])\n        res_dataset.to_csv('output-verbatims-nov-2023-thres08-model3741-2702241340.csv',index = False)\n        print(predictions[0])\n        print(predicted_labels_multilabel[2])\n        print(res_dataset['VERBATIM'][0])\n        print(multiLabelBinarizer.classes_)\n","metadata":{"id":"hrJsiwZ-Q0IA","execution":{"iopub.status.busy":"2024-07-05T06:24:37.184921Z","iopub.execute_input":"2024-07-05T06:24:37.185838Z","iopub.status.idle":"2024-07-05T06:24:49.006678Z","shell.execute_reply.started":"2024-07-05T06:24:37.185805Z","shell.execute_reply":"2024-07-05T06:24:49.005834Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-07-05 06:24:39.028430: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-05 06:24:39.028562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-05 06:24:39.161747: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Obtener predicciones de un dataset completo\n\ntrainer_utils:TrainerUtils = TrainerUtils(\"\")\nmodel_name = \"/kaggle/working/results-bert/checkpoint-3810\"\ndtype:Dict[str,str]={\"VERBATIM\":\"str\",\"CODES\":\"str\"}\ndata_loader:DataLoaderBert = DataLoaderBert(\"/kaggle/input/datase-t/nespresso_reviews - nespresso_reviews (1).csv\",\"CODES\",dtype)\nprint(data_loader.num_labels)\ntrainer_utils.predictDataset(model_name, data_loader = data_loader, threshold = 0.8)","metadata":{"id":"vnDUrrKLQ0IB","execution":{"iopub.status.busy":"2024-07-05T07:47:50.907533Z","iopub.execute_input":"2024-07-05T07:47:50.907878Z","iopub.status.idle":"2024-07-05T07:47:53.666595Z","shell.execute_reply.started":"2024-07-05T07:47:50.907850Z","shell.execute_reply":"2024-07-05T07:47:53.665690Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"40\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['495', '9', '9995'] will be ignored\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/80 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55d44c6b66b41a3843727206044441d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\n('402',)\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n('207',)\nTuve que esperar casi una semana para que recogieran mi máquina para la reparación. Demasiado tiempo.\n['207' '708' '0405' '101' '9203' '0301' '9202' '0702' '406' '9501' '205'\n '0404' '103' '0701' '302' '9301' '0403' '9201' '405' '0501' '701' '203'\n '9401' '301' '0302' '9101' '808' '104' '206' '9801' '9702' '0104' '9601'\n '403' '0207' '201' '9701' '702' '404' '204' '0103' '0201' '202' '501'\n '0102' '0205' '402' '0204' '401' '102' '0208' '0101' '0202' '0401' '0808'\n '0402' '601' '208' '0601' '0203' '0708' '0206' '0406']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Definición de funciones de perdida**","metadata":{"id":"n6yYDD2yQ0IB"}},{"cell_type":"code","source":"from torch.optim import Adam\nfrom torch.nn import BCEWithLogitsLoss\n\n# Define la función de pérdida BCEWithLogitsLoss\nloss_fn = BCEWithLogitsLoss()\n# Crear una función de entrenamiento personalizada\ndef compute_loss(model, inputs):\n    outputs = model(**inputs)\n    logits = outputs.logits\n    labels = inputs[\"labels\"]\n    return loss_fn(logits, labels)\n\n# Sobrescribir el método de entrenamiento\ndef train_step(model, inputs):\n    model.train()\n    loss = compute_loss(model, inputs)\n    loss.backward()\n    return loss.item()","metadata":{"id":"eURVW9NRQ0IB","execution":{"iopub.status.busy":"2024-07-05T06:24:54.855471Z","iopub.execute_input":"2024-07-05T06:24:54.856442Z","iopub.status.idle":"2024-07-05T06:24:54.862835Z","shell.execute_reply.started":"2024-07-05T06:24:54.856410Z","shell.execute_reply":"2024-07-05T06:24:54.861774Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Proceso de entrenamiento**","metadata":{"id":"W170NsEbQ0IC"}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import DataCollatorWithPadding\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding\nfrom tensorboard import program\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\n\nprint(\"Cargamos el dataset\")\ndtype:Dict[str,str]={\"VERBATIM\":\"str\",\"CODES\":\"str\"}\ndataLoader:DataLoaderBert = DataLoaderBert(\"/kaggle/input/dataset-borrados/ds-subtopics-combined-v12-valores_borrados.csv\",\"CODES\",dtype)\ndataLoader_eval:DataLoaderBert = DataLoaderBert(\"/kaggle/input/dataset-borrados/ds-subtopics-combined-v12-valores_borrados.csv\",\"CODES\",dtype)\nprint(f\"num labels: {dataLoader.num_labels} y labels: {dataLoader.labels}\")\n#dataLoader.explain()\nprint(\"Cargamos el modelo BERT\")\nbertModel:BertModel=BertModel(dataLoader.num_labels, \"/kaggle/input/modelo-bert/bert_base\")\n\n# Convertir las etiquetas de texto a etiquetas numéricas\nmultiLabelBinarizer:MultiLabelBinarizer = MultiLabelBinarizer(classes=dataLoader.labels)\ndataLoader.df['labels'] = list(multiLabelBinarizer.fit_transform(dataLoader.df['CODES']).astype(float))\ndataLoader_eval.df['labels'] = list(multiLabelBinarizer.fit_transform(dataLoader_eval.df['CODES']).astype(float))\n#dataset:Dataset = Dataset.from_pandas(dataLoader.df).train_test_split(train_size=0.8, test_size=0.2, seed = 42) # doing the split reproducible\ndataset_train:Dataset = Dataset.from_pandas(dataLoader.df) # doing the split reproducible\ndataset_eval:Dataset = Dataset.from_pandas(dataLoader_eval.df) # doing the split reproducible\n\n# Check the number of records in training and testing dataset.\nprint('The training dataset has', len(dataset_train) ,' records.')\nprint('The test dataset has', len(dataset_eval) ,' records.')\n\n# Tokeniza los textos\ndef tokenize_function(examples):\n    #return bertModel.tokenizer(examples[\"VERBATIM\"], padding='max_length', truncation=True)\n    tokenized_inputs = bertModel.tokenizer(examples[\"VERBATIM\"], padding=\"max_length\", truncation=True, max_length=512)\n     # Añadir etiquetas y convertirlas a flotantes\n    tokenized_inputs[\"labels\"] = [list(map(float, label)) for label in examples[\"labels\"]]\n    return tokenized_inputs\n\n#tokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets_train = dataset_train.map(tokenize_function, batched=True)\ntokenized_datasets_test = dataset_eval.map(tokenize_function, batched=True)\n\n# Utilizar DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=bertModel.tokenizer, return_tensors='pt')\n# Definir los argumentos de entrenamiento\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results-bert',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,#16\n    per_device_eval_batch_size=8,#16\n    warmup_steps=1000,\n    weight_decay=0.01,\n#    learning_rate=1e-5,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    logging_strategy=\"epoch\",\n    logging_dir='/kaggle/working/logs-bert',\n    report_to=\"none\",  # Esto evitará que se registre en W&B APIKEY: caff0a55580191e266efd6f4566148d5bb76129d\n    overwrite_output_dir=False\n)\n\n# Inicializar el Trainer\ntrainer = Trainer(\n    model=bertModel.model,  # Asegúrate de haber cargado o definido tu modelo BERT aquí\n    args=training_args,\n    train_dataset=tokenized_datasets_train,\n    eval_dataset=tokenized_datasets_test,\n    data_collator=data_collator,\n    tokenizer=bertModel.tokenizer,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n    compute_metrics=compute_metrics\n)\n\n# Sobrescribir el método de entrenamiento del Trainer para controlar el algoritmo de perdida y el optimizador de learning rate\ntrainer.train_step = train_step\ntrainer.optimizer = Adam(bertModel.model.parameters(), lr=5e-5) #lr=2e-5\n\n# Entrenar y evaluar\n\n# Carga la extensión de TensorBoard\n#%load_ext tensorboard\n# Inicia TensorBoard\n#%tensorboard --logdir /kaggle/working/logs1\n\n\ntrainer.train()\ntrainer.evaluate()\n\ntrainerUtils:TrainerUtils = TrainerUtils('/kaggle/working/results-bert')\ntrainerUtils.compressBestModel()\nprint(\"best model compressed\")","metadata":{"id":"IzrXIukaQ0IC","execution":{"iopub.status.busy":"2024-07-05T06:25:09.535534Z","iopub.execute_input":"2024-07-05T06:25:09.536191Z","iopub.status.idle":"2024-07-05T07:47:42.514237Z","shell.execute_reply.started":"2024-07-05T06:25:09.536163Z","shell.execute_reply":"2024-07-05T07:47:42.513247Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Cargamos el dataset\nnum labels: 63 y labels: ['207' '708' '0405' '101' '9203' '0301' '9202' '0702' '406' '9501' '205'\n '0404' '103' '0701' '302' '9301' '0403' '9201' '405' '0501' '701' '203'\n '9401' '301' '0302' '9101' '808' '104' '206' '9801' '9702' '0104' '9601'\n '403' '0207' '201' '9701' '702' '404' '204' '0103' '0201' '202' '501'\n '0102' '0205' '402' '0204' '401' '102' '0208' '0101' '0202' '0401' '0808'\n '0402' '601' '208' '0601' '0203' '0708' '0206' '0406']\nCargamos el modelo BERT\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/modelo-bert/bert_base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"The training dataset has 20309  records.\nThe test dataset has 20309  records.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20309 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f837a0059db54307b95a28dda84b4950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20309 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b0a48a36f84d0b9d31fcfcf8fcc2bc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3810' max='3810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3810/3810 1:14:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Roc Auc</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.171700</td>\n      <td>0.068098</td>\n      <td>0.343630</td>\n      <td>0.604037</td>\n      <td>0.081491</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.041100</td>\n      <td>0.020440</td>\n      <td>0.965926</td>\n      <td>0.973911</td>\n      <td>0.924073</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.017700</td>\n      <td>0.013174</td>\n      <td>0.988639</td>\n      <td>0.992045</td>\n      <td>0.977498</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"<transformers.trainer_utils.EvalPrediction object at 0x7b6278429720>\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"<transformers.trainer_utils.EvalPrediction object at 0x7b62681ab910>\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"<transformers.trainer_utils.EvalPrediction object at 0x7b627eb765c0>\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1270' max='1270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1270/1270 06:22]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"<transformers.trainer_utils.EvalPrediction object at 0x7b6268146b60>\nbest model compressed\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Si queremos hacer clean del resultado del entrenamiento","metadata":{"id":"eGp2dCALQ0IC"}},{"cell_type":"code","source":"trainerUtils:TrainerUtils = TrainerUtils('/kaggle/working/results-bert')\ntrainerUtils.compressModel('model-v7-4-checkpoint-weight001lr2e5-3741-2702241330','/kaggle/working/results-bert/checkpoint-2494' )","metadata":{"execution":{"iopub.status.busy":"2024-02-29T17:04:52.450391Z","iopub.execute_input":"2024-02-29T17:04:52.450796Z","iopub.status.idle":"2024-02-29T17:05:59.855612Z","shell.execute_reply.started":"2024-02-29T17:04:52.450764Z","shell.execute_reply":"2024-02-29T17:05:59.85477Z"},"id":"Asc7IcYDQ0IC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainerUtils:TrainerUtils = TrainerUtils('/kaggle/working/results-bert')\nprint(trainerUtils.searchBestCheckpoint())\n#trainerUtils.removeWorstCheckpoint()","metadata":{"id":"lPzsgUjhQ0ID","execution":{"iopub.status.busy":"2024-07-05T08:10:28.658125Z","iopub.execute_input":"2024-07-05T08:10:28.659012Z","iopub.status.idle":"2024-07-05T08:10:28.665077Z","shell.execute_reply.started":"2024-07-05T08:10:28.658979Z","shell.execute_reply":"2024-07-05T08:10:28.664159Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/kaggle/working/results-bert/checkpoint-3810\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tunning a partir del mejor checkpoint\n\nUna vez hemos entrenado un modelo (generalmente hemos entrenando la capa de clasificación de un modelo como BERT,Roberta, etc con nuestros datos), es decir, hemos entrenado la capa de clasificación de un modelo y por tanto hemos obtenido un clasificador con pesos y etiquetas propias, podemos seguir reentrenando (refinando) esa capa a partir de las mejores métricas que hayamos podido conseguir en dicho refinamiento previo. Esto nos permite seguir ajustando más el modelo","metadata":{"id":"IvPBSSEhQ0ID"}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-02-27T12:36:53.364596Z","iopub.execute_input":"2024-02-27T12:36:53.365006Z","iopub.status.idle":"2024-02-27T12:36:53.369395Z","shell.execute_reply.started":"2024-02-27T12:36:53.364973Z","shell.execute_reply":"2024-02-27T12:36:53.368475Z"},"id":"235w6t5_Q0ID","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import DataCollatorWithPadding\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding,AutoModelForSequenceClassification, AutoTokenizer\nfrom tensorboard import program\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\nprint(\"Cargamos el dataset\")\ndtype:Dict[str,str]={\"VERBATIM\":\"str\",\"CODES\":\"str\"}\ndataLoader = DataLoaderBert(\"/kaggle/input/data-train-150-topics/ds-subtopics-combined_v1.csv\",\"CODES\",dtype)\nprint(\"Cargamos el modelo BERT\")\nbertModel:BertModel=BertModel(dataLoader.num_labels)\n\n# Convert pyhton dataframe to Hugging Face arrow dataset\n# Convertir las etiquetas de texto a etiquetas numéricas\nmultiLabelBinarizer:MultiLabelBinarizer = MultiLabelBinarizer(classes=dataLoader.labels)\ndataLoader.df['labels'] = list(multiLabelBinarizer.fit_transform(dataLoader.df['CODES']).astype(float))\ndataset:Dataset = Dataset.from_pandas(dataLoader.df).train_test_split(train_size=0.8, test_size=0.2, seed = 42) # doing the split reproducible\n\n\ntrainerUtils:TrainerUtils = TrainerUtils(\"/kaggle/working/results-retuning\")\nmodel =  AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/results-bert/checkpoint-2494\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/results-bert/checkpoint-2494\")\n\n# Tokeniza los textos\ndef tokenize_function(examples):\n    #return bertModel.tokenizer(examples[\"VERBATIM\"], padding='max_length', truncation=True)\n    tokenized_inputs = bertModel.tokenizer(examples[\"VERBATIM\"], padding=\"max_length\", truncation=True, max_length=512)\n     # Añadir etiquetas y convertirlas a flotantes\n    tokenized_inputs[\"labels\"] = [list(map(float, label)) for label in examples[\"labels\"]]\n    return tokenized_inputs\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Utilizar DataCollatorWithPadding agrupar dinámicamente un lote de ejemplos de datos de manera eficiente.\n# La principal funcionalidad de esta clase es tomar una lista de ejemplos (que pueden tener diferentes longitudes)\n# y combinarlos en un solo lote con padding aplicado, de modo que todos los ejemplos en el lote tengan la misma longitud.\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')\n\n# Definir los argumentos de entrenamiento\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results-retuning',\n    num_train_epochs=15,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.1,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    #learning_rate=1e-4,\n    load_best_model_at_end=True,\n    logging_strategy=\"epoch\",\n    logging_dir='/kaggle/working/logs-retuning',\n    report_to=\"none\",  # Esto evitará que se registre en W&B APIKEY: caff0a55580191e266efd6f4566148d5bb76129d\n    overwrite_output_dir=False\n)\n\n# Inicializar el Trainer\ntrainer = Trainer(\n    model=model,  # Asegúrate de haber cargado o definido tu modelo BERT aquí\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n    compute_metrics=compute_metrics\n)\n\n# Sobrescribir el método de entrenamiento del Trainer para controlar el algoritmo de perdida y el optimizador de learning rate\ntrainer.train_step = train_step\ntrainer.optimizer = Adam(bertModel.model.parameters(), lr=2e-5)\ntrainer.train()\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T08:00:56.20526Z","iopub.execute_input":"2024-01-10T08:00:56.205618Z","iopub.status.idle":"2024-01-10T08:01:19.034087Z","shell.execute_reply.started":"2024-01-10T08:00:56.205589Z","shell.execute_reply":"2024-01-10T08:01:19.032765Z"},"id":"z9t8LubhQ0ID","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainerUtils:TrainerUtils = TrainerUtils('/kaggle/working/results-bert')\nprint(trainerUtils.searchBestCheckpoint())\n#trainerUtils.removeWorstCheckpoint()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T11:06:29.726119Z","iopub.execute_input":"2023-12-13T11:06:29.72677Z","iopub.status.idle":"2023-12-13T11:06:30.405133Z","shell.execute_reply.started":"2023-12-13T11:06:29.726735Z","shell.execute_reply":"2023-12-13T11:06:30.404164Z"},"id":"yOj_OkCXQ0ID","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**save model**","metadata":{"id":"lvOtnMAaQ0IE"}},{"cell_type":"code","source":"from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name = \"dccuchile/bert-base-spanish-wwm-cased\"\npath:str = \"/kaggle/working/results-bert/checkpoint-1502\"\n#path:str = \"/kaggle/working/results-retuning/checkpoint-297\"\nconfig = BertConfig.from_pretrained(path)\nmodel =  AutoModelForSequenceClassification.from_pretrained(path,config=config)\ntokenizer = AutoTokenizer.from_pretrained(path)\nmodel.save_pretrained(\"/kaggle/working/modelos/nes-multilabel-v5-epoch2-19120948\")\nconfig.save_pretrained(\"/kaggle/working/modelos/nes-multilabel-v5-epoch2-19120948\")\ntokenizer.save_pretrained(\"/kaggle/working/modelos/nes-multilabel-v5-epoch2-19120948\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-19T08:48:29.165798Z","iopub.execute_input":"2023-12-19T08:48:29.16675Z","iopub.status.idle":"2023-12-19T08:48:31.11173Z","shell.execute_reply.started":"2023-12-19T08:48:29.166712Z","shell.execute_reply":"2023-12-19T08:48:31.110813Z"},"id":"1Nk22bA9Q0IE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Guardar a zip\ntrainerUtils:TrainerUtils = TrainerUtils('/kaggle/working/results-bert')\ntrainerUtils.compressModel(\"nes-multilabel-v5-epoch6-18121830\", \"/kaggle/working/modelos/nes-multilabel-v5-epoch6-18121830\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T17:35:29.590856Z","iopub.execute_input":"2023-12-18T17:35:29.591633Z","iopub.status.idle":"2023-12-18T17:35:54.748407Z","shell.execute_reply.started":"2023-12-18T17:35:29.591596Z","shell.execute_reply":"2023-12-18T17:35:54.74749Z"},"id":"tS6SufLWQ0IE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test with trainer\nTest loading a trained model from models or checkpoint","metadata":{"id":"1KStTz_gQ0IE"}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import DataCollatorWithPadding\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom transformers import Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding,AutoModelForSequenceClassification, AutoTokenizer\nfrom tensorboard import program\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nprint(\"Cargamos el dataset\")\ndtype:Dict[str,str]={\"VERBATIM\":\"str\",\"CODES\":\"str\"}\ndataLoader:DataLoaderBert = DataLoaderBert(\"/kaggle/input/data-train-all-categories/ds-subtopics-combined-v3.csv\",\"CODES\",dtype)\ndataLoader_eval:DataLoaderBert = DataLoaderBert(\"/kaggle/input/data-train-all-categories/ds-test-multilabel-v1.csv\",\"CODES\",dtype)\nprint(f\"num labels: {dataLoader.num_labels} y labels: {dataLoader.labels}\")\n#dataLoader.explain()\nprint(\"Cargamos el modelo BERT\")\nbertModel:BertModel=BertModel(dataLoader.num_labels, model_name = \"/kaggle/working/modelos/nes-multilabel-v3-epoch4-15121057\")\n\n# Convertir las etiquetas de texto a etiquetas numéricas\nmultiLabelBinarizer:MultiLabelBinarizer = MultiLabelBinarizer(classes=dataLoader.labels)\ndataLoader.df['labels'] = list(multiLabelBinarizer.fit_transform(dataLoader.df['CODES']).astype(float))\ndataLoader_eval.df['labels'] = list(multiLabelBinarizer.fit_transform(dataLoader_eval.df['CODES']).astype(float))\n#dataset:Dataset = Dataset.from_pandas(dataLoader.df).train_test_split(train_size=0.8, test_size=0.2, seed = 42) # doing the split reproducible\ndataset_train:Dataset = Dataset.from_pandas(dataLoader.df) # doing the split reproducible\ndataset_eval:Dataset = Dataset.from_pandas(dataLoader_eval.df) # doing the split reproducible\n\n# Check the number of records in training and testing dataset.\nprint('The training dataset has', len(dataset_train) ,' records.')\nprint('The test dataset has', len(dataset_eval) ,' records.')\n\n# Tokeniza los textos\ndef tokenize_function(examples):\n    #return bertModel.tokenizer(examples[\"VERBATIM\"], padding='max_length', truncation=True)\n    tokenized_inputs = bertModel.tokenizer(examples[\"VERBATIM\"], padding=\"max_length\", truncation=True, max_length=512)\n     # Añadir etiquetas y convertirlas a flotantes\n    tokenized_inputs[\"labels\"] = [list(map(float, label)) for label in examples[\"labels\"]]\n    return tokenized_inputs\n\n#tokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets_train = dataset_train.map(tokenize_function, batched=True)\ntokenized_datasets_test = dataset_eval.map(tokenize_function, batched=True)\n\n# Utilizar DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=bertModel.tokenizer, return_tensors='pt')\n\n\n# Definir los argumentos de entrenamiento\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results-retuning',\n    num_train_epochs=15,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.1,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    #learning_rate=1e-4,\n    load_best_model_at_end=True,\n    logging_strategy=\"epoch\",\n    logging_dir='/kaggle/working/logs-retuning',\n    report_to=\"none\",  # Esto evitará que se registre en W&B APIKEY: caff0a55580191e266efd6f4566148d5bb76129d\n    overwrite_output_dir=False\n)\n\n# Inicializar el Trainer\ntrainer = Trainer(\n    model= bertModel.model,  # Asegúrate de haber cargado o definido tu modelo BERT aquí\n    args=training_args,\n    train_dataset=tokenized_datasets_train,\n    eval_dataset=tokenized_datasets_test,\n    data_collator=data_collator,\n    tokenizer=bertModel.tokenizer,\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n    compute_metrics=compute_metrics\n)\n\n\nraw_pred, labels, _ = trainer.predict(tokenized_datasets_test)\n\n# Preprocess raw predictions\ny_pred = np.argmax(raw_pred, axis=1)\n\n\nlabels_list = dataLoader.labels\nprint(raw_pred)\nprint(labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:03:36.080052Z","iopub.execute_input":"2023-12-15T11:03:36.081042Z","iopub.status.idle":"2023-12-15T11:03:37.429545Z","shell.execute_reply.started":"2023-12-15T11:03:36.081002Z","shell.execute_reply":"2023-12-15T11:03:37.428334Z"},"id":"V1qImgRzQ0IE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bertModel:BertModel=BertModel(dataLoader.num_labels, model_name = \"/kaggle/working/results-bert/checkpoint-3810\")\n\n\ntext = \"La repartidora se retrasó mucho, muy malo todo\"\n\nencoding = bertModel.tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n#encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n\noutputs = trainer.model(**encoding)\nlogits = outputs.logits\numbral = 0.7\n# apply sigmoid + threshold\nsigmoid = torch.nn.Sigmoid()\nprobs = sigmoid(logits.squeeze().cpu())\npredictions = np.zeros(probs.shape)\npredictions[np.where(probs >= umbral)] = 1\nprint(predictions)\n\n# indices_1 = [i for i, value in enumerate(predictions) if value == 1]\n# # turn predicted id's into actual label names\n\n# print(labels[indices_1])\n\nres_list = []\nfor i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n    res = []\n\nthemes = predictText(sentence)\nabsa_prediction = classifier_ABSA(sentence,  text_pair=aspect)\nsentiment = sentiment_to_number(absa_prediction[0]['label'])\ntheme_code = label_to_code(aspect)\ncode = create_code(sentiment, theme_code)\nif code not in res:\n    res.append(code)\nres_list.append(res)\ndataset['CLASSIFICATION'] = res_list\ndataset.to_excel('output3.xlsx', index=False)\nprint('Text analysis finished')\n","metadata":{"id":"NPyQEZUrQ0IF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validar encoding casos concretos","metadata":{"id":"HDOV0_BNQ0IF"}},{"cell_type":"code","source":"def printTokens(sentence:str, tokenizer:AutoTokenizer):\n    # Tokeniza la frase\n    inputs = tokenizer(sentence)\n\n    # Imprime los tokens\n    print(f\"Tokens: {inputs.tokens()}\")\n\n    # Imprime los IDs de los tokens\n    print(f\"Token IDs: {inputs['input_ids']}\")\n\n    # Imprime los tokens correspondientes a cada ID\n    print(\"Mapeo de tokens a palabras:\")\n    for token_id in inputs['input_ids']:\n        print(f\"{token_id} ----> {tokenizer.decode([token_id])}\")","metadata":{"id":"36IU6IluQ0IF","execution":{"iopub.status.busy":"2024-07-05T08:15:21.698677Z","iopub.execute_input":"2024-07-05T08:15:21.699024Z","iopub.status.idle":"2024-07-05T08:15:21.704385Z","shell.execute_reply.started":"2024-07-05T08:15:21.698996Z","shell.execute_reply":"2024-07-05T08:15:21.703540Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model_name=\"/kaggle/working/results-bert/checkpoint-3810\"\ntrainerUtils:TrainerUtils = TrainerUtils(model_name)\n#trainerUtils.removeWorstCheckpoint()\n#trainerUtils.saveBestCheckpoint(\"/kaggle/working/backup-best-checkpoint-retrain\")","metadata":{"id":"AVcCXYNwQ0IF","execution":{"iopub.status.busy":"2024-07-05T08:12:33.455689Z","iopub.execute_input":"2024-07-05T08:12:33.456314Z","iopub.status.idle":"2024-07-05T08:12:33.460803Z","shell.execute_reply.started":"2024-07-05T08:12:33.456283Z","shell.execute_reply":"2024-07-05T08:12:33.459741Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\ndtype:Dict[str,str]={\"VERBATIM\":\"str\",\"CODES\":\"str\"}\ndataLoader = DataLoaderBert(\"/kaggle/input/data-train-all-categories/ds-subtopics-combined-v6.csv\",\"CODES\",dtype)\nmultiLabelBinarizer:MultiLabelBinarizer = MultiLabelBinarizer(classes=dataLoader.labels)\ndataLoader.df['labels'] = list(multiLabelBinarizer.fit_transform(dataLoader.df['CODES']).astype(float))","metadata":{"execution":{"iopub.status.busy":"2023-12-21T11:32:42.646471Z","iopub.execute_input":"2023-12-21T11:32:42.647165Z","iopub.status.idle":"2023-12-21T11:32:42.770301Z","shell.execute_reply.started":"2023-12-21T11:32:42.647133Z","shell.execute_reply":"2023-12-21T11:32:42.769343Z"},"id":"dBt5Bd9IQ0IF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom typing import Optional,Dict\nfrom transformers import BertConfig, BertForSequenceClassification, BertTokenizer, AutoModelForSequenceClassification, AutoTokenizer\n\ntext = \"Las cápsulas a veces no funcionan bien en la máquina y terminan haciendo un desastre.\"\n\n\n# Configuración personalizada del modelo\nconfig = BertConfig.from_pretrained(model_name)\n#config.num_attention_heads = 16  # Configurar el número de cabezas de atención\n#config.hidden_size = 1024  # Configurar el tamaño de la capa oculta\n\nmodel =  AutoModelForSequenceClassification.from_pretrained(model_name,config=config)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n\n# Realizar la predicción\noutputs = model(**inputs)\n# Aplicar softmax para obtener probabilidades\nprobs = torch.sigmoid(outputs.logits)\n# Umbral para decidir si una etiqueta está presente o no\nthreshold = 0.5 # Puedes ajustar este umbral según tus necesidades\n\n# Obtener las etiquetas predichas basadas en el umbral\npredicted_labels = (probs > threshold).cpu().numpy()\npredicted_labels_multilabel = multiLabelBinarizer.inverse_transform(predicted_labels)\n# Mostrar las probabilidades multilabel\nprint(probs)\nprint(multiLabelBinarizer.classes_)\n# Mostrar las etiquetas predichas\nprint(f\"Etiquetas predichas: {predicted_labels_multilabel[0]}\")\n\n#labels: ['0403' '0301' '0501' '9101' '0101' '0702' '0406' '0203' '9601' '0208'\n #'0402' '0207' '9301' '0201' '0404' '0708' '0405' '0401' '0202' '9701'\n #'0701' '0206' '9200' '0204' '0103' '0601' '9401' '0205' '9801' '0104'\n #'0302' '9501' '0102' '9201' '0808' '9203' '9702']\n","metadata":{"id":"xGOYQrgmQ0IG","execution":{"iopub.status.busy":"2024-07-05T08:31:43.135608Z","iopub.execute_input":"2024-07-05T08:31:43.136185Z","iopub.status.idle":"2024-07-05T08:31:43.381110Z","shell.execute_reply.started":"2024-07-05T08:31:43.136156Z","shell.execute_reply":"2024-07-05T08:31:43.380035Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"tensor([[8.8046e-01, 1.8520e-03, 1.7719e-03, 2.4689e-03, 2.1031e-03, 2.6388e-03,\n         4.4979e-03, 1.5831e-03, 1.9842e-03, 2.2240e-03, 2.2830e-03, 3.1887e-03,\n         2.5453e-03, 2.1705e-03, 8.2217e-03, 2.4668e-03, 2.3898e-03, 4.0829e-03,\n         4.0544e-03, 2.8820e-03, 2.2925e-03, 2.1022e-03, 3.7440e-03, 2.1657e-03,\n         2.2478e-03, 2.3617e-03, 3.1354e-03, 2.8055e-03, 5.3580e-03, 2.4822e-03,\n         2.1487e-03, 2.2310e-03, 3.8375e-03, 1.8431e-03, 7.7863e-03, 3.1924e-03,\n         2.8614e-03, 6.9124e-04, 9.0203e-04, 4.7381e-03, 3.0513e-03, 3.7595e-03,\n         4.0621e-03, 3.5735e-03, 1.3543e-03, 3.9845e-03, 1.7107e-03, 1.5908e-03,\n         6.4490e-03, 1.4058e-03, 3.1207e-03, 2.1875e-03, 3.4279e-03, 2.7685e-03,\n         4.1419e-03, 3.0725e-03, 3.6200e-03, 2.7959e-02, 2.6013e-03, 2.9844e-03,\n         2.5114e-03, 3.0903e-03, 4.5037e-03]], grad_fn=<SigmoidBackward0>)\n['207' '708' '0405' '101' '9203' '0301' '9202' '0702' '406' '9501' '205'\n '0404' '103' '0701' '302' '9301' '0403' '9201' '405' '0501' '701' '203'\n '9401' '301' '0302' '9101' '808' '104' '206' '9801' '9702' '0104' '9601'\n '403' '0207' '201' '9701' '702' '404' '204' '0103' '0201' '202' '501'\n '0102' '0205' '402' '0204' '401' '102' '0208' '0101' '0202' '0401' '0808'\n '0402' '601' '208' '0601' '0203' '0708' '0206' '0406']\nEtiquetas predichas: ('207',)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Por probabilidades más altas descartando siempre cada alta dectada","metadata":{"id":"CsJDlYdXQ0IG"}},{"cell_type":"code","source":"etiquetas_seleccionadas = []  # Lista para almacenar las etiquetas seleccionadas\n\n\nwhile True:\n    # Aplicar la función softmax\n    probabilities = torch.softmax(outputs.logits, dim=1).detach().numpy()\n    print(probabilities)\n    # Verificar si alguna probabilidad supera el umbral\n    if np.any(probabilities > threshold):\n        # Encontrar la etiqueta con la probabilidad más alta\n        etiqueta_maxima = np.argmax(probabilities)\n        # Agregar la etiqueta máxima a la lista de etiquetas seleccionadas\n        etiquetas_seleccionadas.append(etiqueta_maxima)\n        print(outputs.logits)\n        # Establecer la probabilidad de la etiqueta máxima a cero para la próxima iteración\n        outputs.logits[0][etiqueta_maxima]  = float('-inf')\n    else:\n        print(\"break\")\n        break\n# Mostrar las etiquetas seleccionadas\nprint(\"Etiquetas seleccionadas:\", etiquetas_seleccionadas)","metadata":{"id":"QMO-9hivQ0IG","execution":{"iopub.status.busy":"2024-07-05T08:14:28.577270Z","iopub.execute_input":"2024-07-05T08:14:28.577981Z","iopub.status.idle":"2024-07-05T08:14:28.587293Z","shell.execute_reply.started":"2024-07-05T08:14:28.577951Z","shell.execute_reply":"2024-07-05T08:14:28.586185Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[[0.00242094 0.00453019 0.00207138 0.01739279 0.00152058 0.00156386\n  0.00144701 0.00326289 0.00369448 0.00463661 0.00306001 0.00135134\n  0.00312196 0.00221542 0.00131441 0.00196833 0.00182151 0.0083843\n  0.00177623 0.0032347  0.00204488 0.00292515 0.00213358 0.00128112\n  0.00294682 0.01427312 0.00177721 0.33766976 0.00225533 0.04066738\n  0.00145877 0.01801542 0.01075978 0.00359346 0.0017441  0.00347106\n  0.00630079 0.00114604 0.00154241 0.00444814 0.00362186 0.00457922\n  0.00291331 0.00363794 0.00482112 0.00164405 0.00335969 0.00187216\n  0.00203447 0.00497342 0.00114434 0.40852788 0.00266815 0.0020139\n  0.0022627  0.00173538 0.00212553 0.00313235 0.00251436 0.00152021\n  0.00201095 0.0012305  0.00441313]]\nbreak\nEtiquetas seleccionadas: []\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokeniza la frase\ntokens = tokenizer.tokenize(text)\n# Imprime los tokens resultantes\nprint(tokens)\n# Convierte los tokens a IDs\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n# Imprime los IDs de los tokens\nprint(token_ids)","metadata":{"id":"Q7sHmacxQ0IL","execution":{"iopub.status.busy":"2024-07-05T08:14:35.384923Z","iopub.execute_input":"2024-07-05T08:14:35.385345Z","iopub.status.idle":"2024-07-05T08:14:35.391869Z","shell.execute_reply.started":"2024-07-05T08:14:35.385313Z","shell.execute_reply":"2024-07-05T08:14:35.390959Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"['Ten', '##eis', 'muy', 'buena', 'atención', 'al', 'cliente', ',', 'aun', '##q', 'a', 'veces', 'las', 'empresa', 'de', 'mensaje', '##ría', '(', 'Se', '##ur', ')', 'deja', 'mucho', 'q', 'desear', ':', 'bastantes', 'veces', 'las', 'cápsulas', 'vienen', 'abol', '##ladas']\n[1599, 19211, 1456, 2667, 3148, 1091, 6114, 1017, 6314, 30951, 1013, 2397, 1089, 2784, 1008, 4837, 2623, 1147, 1264, 1284, 1135, 5113, 1789, 1033, 21704, 1181, 23366, 2397, 1089, 29699, 7690, 14840, 9299]\n","output_type":"stream"}]},{"cell_type":"code","source":"printTokens(\"Cafeteras y café de calidad y el pedido lo entregaron a tiempo\", tokenizer)","metadata":{"id":"EDe6Yf80Q0IL","execution":{"iopub.status.busy":"2024-07-05T08:15:27.611566Z","iopub.execute_input":"2024-07-05T08:15:27.612157Z","iopub.status.idle":"2024-07-05T08:15:27.618978Z","shell.execute_reply.started":"2024-07-05T08:15:27.612129Z","shell.execute_reply":"2024-07-05T08:15:27.617943Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Tokens: ['[CLS]', 'Ca', '##fete', '##ras', 'y', 'café', 'de', 'calidad', 'y', 'el', 'pedido', 'lo', 'entregar', '##on', 'a', 'tiempo', '[SEP]']\nToken IDs: [4, 1906, 24710, 1267, 1042, 5649, 1008, 3285, 1042, 1040, 7345, 1114, 7538, 1022, 1013, 1577, 5]\nMapeo de tokens a palabras:\n4 ----> [CLS]\n1906 ----> Ca\n24710 ----> ##fete\n1267 ----> ##ras\n1042 ----> y\n5649 ----> café\n1008 ----> de\n3285 ----> calidad\n1042 ----> y\n1040 ----> el\n7345 ----> pedido\n1114 ----> lo\n7538 ----> entregar\n1022 ----> ##on\n1013 ----> a\n1577 ----> tiempo\n5 ----> [SEP]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LIME\n\nEs una librería que nos permite analizar como el modelo asigna los diferentes pesos a las palabras de una frase para determinar que etiqueta asigna","metadata":{"id":"NhTmlh58Q0IL"}},{"cell_type":"code","source":"!pip install lime","metadata":{"id":"2uoI-EYJQ0IL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para disponer de las label, cargamos el dataset. Ejecutamos Lime con una frase de ejemEjecutamos Lime con una frase de ejem","metadata":{"id":"md-7HLwLQ0IM"}},{"cell_type":"code","source":"import torch\nfrom lime.lime_text import LimeTextExplainer\n\n# Cargamos el mejor modelo que tengamos entrenado\nmodel_name=\"/kaggle/working/results-retuning/checkpoint-388\"\nmodel =  AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef predict_fn(texts):\n    # Asegúrate de que 'texts' sea una lista de textos\n    if not isinstance(texts, list):\n        texts = [texts]\n\n    # Preparar la entrada para el modelo BERT\n    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n\n    # Realiza las predicciones con el modelo BERT\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    probabilities = torch.sigmoid(logits)\n    #probabilities = torch.softmax(logits, dim=1).cpu().numpy() # Softmax se aplica a lo largo del eje de las clases\n    return probabilities\n\n# Crea el explainer de LIME\nexplainer = LimeTextExplainer(class_names=multiLabelBinarizer.classes_)\n\n# Ahora, suponiendo que tienes un texto de prueba llamado 'texto_de_prueba'\ntexto_de_prueba = \"Buena atencion en la tienda y facilidad en la web.\"\n\n# Explica la instancia de texto\nexplanation = explainer.explain_instance(texto_de_prueba, predict_fn, num_features=10)\n\n# Muestra la explicación en el cuaderno\n# Esta parte solo funcionará en un entorno de Jupyter Notebook\nexplanation.show_in_notebook()","metadata":{"id":"p0hQsMeyQ0IM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lime multilabel","metadata":{"id":"QUA_3_YhQ0IM"}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom lime.lime_text import LimeTextExplainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Cargamos el mejor modelo que tengamos entrenado\nmodel_name = \"/kaggle/working/results-retuning/checkpoint-388\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef predict_fn(texts):\n    # Asegúrate de que 'texts' sea una lista de textos\n    if not isinstance(texts, list):\n        texts = [texts]\n\n    # Preparar la entrada para el modelo BERT\n    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n\n    # Realiza las predicciones con el modelo BERT\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    probabilities = torch.sigmoid(logits)\n    return probabilities\n\n# Crear el explainer de LIME\nexplainer = LimeTextExplainer(class_names=list(map(str, range(37))))  # Cambiar 'num_labels' al número correcto de etiquetas\n\n# Ahora, suponiendo que tienes un texto de prueba llamado 'texto_de_prueba'\ntexto_de_prueba = \"Buena atención en la tienda y facilidad en la web.\"\n\n# Explicar la instancia de texto\nexplanation = explainer.explain_instance(texto_de_prueba, predict_fn, num_features=10)\n\n# Mostrar las características más importantes en la explicación\n#explanation.show_in_notebook()\n# Obtener las características explicadas e importancias\nexplained_features = explanation.as_list()\nexplained_features = [f[0] for f in explained_features]\nimportances = [f[1] for f in explained_features]\n\n# Mostrar las características explicadas e importancias en una celda de Markdown\nprint(\"Características explicadas:\")\nfor feature in explained_features:\n    print(f\"- {feature}\")\n\nprint(\"\\nImportancias:\")\nfor importance in importances:\n    print(f\"- {importance:.4f}\")\n","metadata":{"id":"uMfzaVxeQ0IM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(explained_features)\nimportances = [f[1] for f in explained_features]\n\nprint(importances)","metadata":{"id":"-hR8Xa1KQ0IM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLTK\n\nEjemplos nltk para inyectar sentencias SEP en una oración, la idea de esto es adaptar las oraciones para que el modelo interprete partes de la oración y no todo como una oración","metadata":{"id":"m-yeI1yrQ0IN"}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\n\n# Descargar el paquete punkt solo la primera vez\n# nltk.download('punkt')\n\ntext = \"Buena atención en la tienda y facilidad en la web. Me gusta viajar en moto\"\n\n# Tokenizar el texto en oraciones\nsentences = sent_tokenize(text, language='spanish')\n\n# Añadir el token [SEP] entre las oraciones\nsep_token = \" [SEP] \"\ntokenized_text_with_sep = sep_token.join(sentences)\nprint(tokenized_text_with_sep)","metadata":{"id":"dxRVLCEVQ0IN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Estos son los tipos semánticos que detecta nltk dentro de una oración, pueden servir para determinar cuando una palabra está actuando dentro de la oración con una forma sintáctica diferente y por tanto podamos determinar que hacemos con ella\n\n* **CC:** Coordinating conjunction (y, but, or)\n* **CD:** Cardinal number (one, two, 3)\n* **DT:** Determiner (the, a, some, most)\n* **EX:** Existential there (there's, there is)\n* **FW:** Foreign word (déjà vu, naiveté)\n* **IN:** Preposition or subordinating conjunction (of, on, before, unless)\n* **JJ:** Adjective (happy, sad)\n* **JJR:** Adjective, comparative (happier, sadder)\n* **JJS:** Adjective, superlative (happiest, saddest)\n* **LS:** List item marker (1), (2), (A), (B)\n* **MD:** Modal (can, could, shall, should)\n* **NN:** Noun, singular or mass (dog, city)\n* **NNS:** Noun, plural (dogs, cities)\n* **NNP:** Proper noun, singular (London, John)\n* **NNPS:** Proper noun, plural (Americans, Canadians)\n* **PDT:** Predeterminer (all, both, half)\n* **POS:** Possessive ending ('s)\n* **PRP:** Personal pronoun (I, you, he)\n* **PRP$:** Possessive pronoun (my, your, his)\n* **RB:** Adverb (very, silently)\n* **RBR:** Adverb, comparative (better)\n* **RBS:** Adverb, superlative (best)\n* **RP:** Particle (up, off)\n* **SYM:** Symbol (+, %, &)\n* **TO:** Infinitive 'to' (to go, to eat)\n* **UH:** Interjection (ah, oops)\n* **VB:** Verb, base form (take, live)\n* **VBD:** Verb, past tense (took, lived)\n* **VBG:** Verb, gerund or present participle (taking, living)\n* **VBN:** Verb, past participle (taken, lived)\n* **VBP:** Verb, non-3rd person singular present (take, live)\n* **VBZ:** Verb, 3rd person singular present (takes, lives)\n* **WDT:** Wh-determiner (which, that)\n* **WP:** Wh-pronoun (who, what)\n* **WP\\$:** Possessive wh-pronoun (whose)\n* **WRB:** Wh-adverb (where, when)","metadata":{"id":"Wv3dhsD4Q0IN"}},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger')\n\n# Ejemplo de oración\ntext = \"Buena atención en la tienda y facilidad en la web.\"\n\n# Tokenizamos la oración en palabras\nwords = nltk.word_tokenize(text)\n\n# Realizamos el POS tagging a la lista de palabras\npos_tags = nltk.pos_tag(words, lang='spanish')\n\n# Buscar la función de \"y\" en la oración\nfor word, tag in pos_tags:\n    if word == \"y\":\n        print(f\"La palabra '{word}' es una {tag}\")\n","metadata":{"id":"YlMrJojaQ0IN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spacy\n\nSpacy utiliza un conjunto de etiquetas para el etiquetado de partes del discurso (POS) que se basan en el Universal Dependencies scheme. Aquí hay una lista de las etiquetas más comunes que encontrarás en Spacy:\n","metadata":{"id":"vXXRLsfKQ0IO"}},{"cell_type":"markdown","source":"* **ADJ:** Adjetivo\n* **ADP:** Adposición\n* **ADV:** Adverbio\n* **AUX:** Auxiliar\n* **CONJ:** Conjunción\n* **CCONJ:** Conjunción coordinante\n* **DET:** Determinante\n* **INTJ:** Interjección\n* **NOUN:** Sustantivo\n* **NUM:** Numeral\n* **PART:** Partícula\n* **PRON:** Pronombre\n* **PROPN:** Nombre propio\n* **PUNCT:** Puntuación\n* **SCONJ:** Conjunción subordinante\n* **SYM:** Símbolo\n* **VERB:** Verbo\n* **X:** Otro","metadata":{"id":"v_xClFMCQ0IO"}},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download es_core_news_sm","metadata":{"id":"DXTls2uzQ0IO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport spacy\nnlp = spacy.load(\"es_core_news_sm\")\n\ntext = \"Buena atención en la tienda y facilidad en la web. Compre la cafetera y las capsulas\"\ndoc = nlp(text)\n\nfor token in doc:\n    if token.text == \"y\":\n        print(f\"La palabra '{token.text}' es una {token.pos_}\")","metadata":{"id":"23LsjCsyQ0IO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Longformer para sumarizar los verbatims","metadata":{"id":"edG-Y9uAQ0IO"}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Cargar el tokenizador y el modelo preentrenado\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n\n\n# Tokenizar el texto y generar el resumen\n# input_ids = tokenizer.encode(texto, return_tensors=\"pt\", max_length=1024, truncation=True)\n# resumen_ids = model.generate(input_ids, max_length=700, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n\n# Decodificar y mostrar el resumen\n# resumen = tokenizer.decode(resumen_ids[0], skip_special_tokens=True)\n# print(resumen)\n","metadata":{"id":"--84U45DQ0IP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bertviz\nhttps://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing#scrollTo=TG-dQt3NOlub","metadata":{"id":"OuuPoWu7Q0IP"}},{"cell_type":"code","source":"!pip install bertviz","metadata":{"id":"vwt-O2tUQ0IP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model and retrieve attention weights\nfrom bertviz import head_view, model_view\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n\n# Carga el modelo y el tokenizador con la configuración para devolver los pesos de atención\nmodel_name_or_path = \"/kaggle/working/results-retuning/checkpoint-388\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, output_attentions=True, problem_type=\"multi_label_classification\")\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n# Prepara las entradas y realiza la predicción\nsentence = \"Buena atencion en la tienda y facilidad en la web. No me gusto la atención por teléfono\"\ninputs = tokenizer(sentence, return_tensors='pt', add_special_tokens=True,max_length=512)\noutputs = model(**inputs)\n\n# Extrae los pesos de atención\nattention = outputs.attentions\n\n# Verifica las dimensiones\nif attention is not None and all(att.shape == (1, model.config.num_attention_heads, model.config.max_position_embeddings, model.config.max_position_embeddings) for att in attention):\n    print(\"Las dimensiones de los tensores de atención son correctas\")\nelse:\n    print(\"Las dimensiones de los tensores de atención NO son correctas\")\n\n# Si las dimensiones son correctas, intenta visualizar\nif attention is not None:\n    from bertviz import head_view\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n    head_view(attention, tokens)\n    model_view(attention, tokens)","metadata":{"id":"Zj-_fdQqQ0IP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import torch\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndef evaluate_model_from_file(model, tokenizer, device, epoch, num_classes, lr):\n\n    texts_val, labels_val, numLabels = loadDatasetCsv('/kaggle/input/data-train-csv/all_data.csv')\n    criterion = torch.nn.CrossEntropyLoss()\n\n    # Tokenizar y convertir a tensores\n    input_ids_val = []\n    attention_masks_val = []\n\n    for text in texts_val:\n        inputs_val = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n        input_ids_val.append(inputs_val[\"input_ids\"].squeeze())\n        attention_masks_val.append(inputs_val[\"attention_mask\"].squeeze())\n\n    input_ids_val = torch.stack(input_ids_val)\n    attention_masks_val = torch.stack(attention_masks_val)\n\n    # Restar 1 a las etiquetas para ajustarlas a un rango [0, 1, 2, ..., 16]\n    labels_val = [label - 1 for label in labels_val]\n    labels_val = torch.tensor(labels_val)\n\n    val_data = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n    val_dataloader = DataLoader(val_data, batch_size=16)\n\n    val_loss = 0.0\n    val_preds = []\n    val_labels = []\n\n    model.eval()  # Cambia el modo del modelo a evaluación\n\n    with torch.no_grad():\n        for batch in val_dataloader:\n                batch = tuple(t.to(device) for t in batch)\n                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n                outputs = model(**inputs)\n                logits = outputs.logits\n                val_loss += criterion(logits, batch[2]).item()\n                val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n                val_labels.extend(batch[2].cpu().numpy())\n\n    val_loss /= len(val_dataloader)\n    classification_metrics = classification_report(val_labels, val_preds, target_names=[str(i) for i in range(num_classes)])\n    conf_matrix = confusion_matrix(val_labels, val_preds)\n\n    metrics_path = model_name + '.txt'\n    with open(metrics_path, 'w') as f:\n        # Redirigir la salida de print al archivo\n        print(f\"Epoch {epoch + 1} Validation Loss: {val_loss}  lr: {lr}\", file=f)\n        print('F1-score: {f1_score}'.format(f1_score = f1_score), file=f)\n        print(\"Classification Report:\\n\", classification_metrics, file=f)\n        print(\"Confusion Matrix:\\n\", conf_matrix, file=f)\n    print(f\"Epoch {epoch + 1} Validation Loss: {val_loss} lr: {lr}\")\n    print(\"Classification Report:\\n\", classification_metrics)\n    print(\"Confusion Matrix:\\n\", conf_matrix)\n\n    model.train()  # Cambia el modo del modelo de nuevo a entrenamiento\"\"\"","metadata":{"id":"Vlg7c1ekQ0IP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import torch\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport sys\n\n#file_path = \"/kaggle/working/log.txt\"\n#sys.stdout = open(file_path, \"w\")\n\ndef evaluate_model_from_train(model, tokenizer, val_dataloader, device, epoch, num_classes,lr):\n    val_loss = 0.0\n    val_preds = []\n    val_labels = []\n    criterion = torch.nn.CrossEntropyLoss()\n\n    # Cambia el modo del modelo a evaluación\n    model.eval()\n\n    with torch.no_grad():\n        for batch in val_dataloader:\n            batch = tuple(t.to(device) for t in batch)\n            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n            outputs = model(**inputs)\n            logits = outputs.logits\n            val_loss += criterion(logits, batch[2]).item()\n            val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n            val_labels.extend(batch[2].cpu().numpy())\n\n    val_loss /= len(val_dataloader)\n    classification_metrics = classification_report(val_labels, val_preds, target_names=[str(i) for i in range(num_classes)])\n    conf_matrix = confusion_matrix(val_labels, val_preds)\n    print(f\"Epoch {epoch + 1} Validation Loss: {val_loss} lr: {lr}\")\n    print(\"Classification Report:\\n\", classification_metrics)\n    print(\"Confusion Matrix:\\n\", conf_matrix)\n\n    # Cambia el modo del modelo de nuevo a entrenamiento\n    model.train()  \"\"\"","metadata":{"id":"j6qTkJCpQ0IP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_epochs=2\n    #lr_values=[\"2e-5\",\"3e-5\",\"4e-5\",\"5e-5\"]\n    lr_values=[\"3e-5\"]\n    weight_decay_values=[\"0.1\",\"0.5\",\"0.0001\"]\n\n    for lr in lr_values:\n        for weight_decay in weight_decay_values:\n            model,tokenizer = loadModelBert()\n            train_data, train_dataloader, val_data, val_dataloader, numLabels = splitDataTrain('/kaggle/input/data-train-150-topics/data_train_400_topics_v3.csv')\n\n            # Definir optimizador y función de pérdida\n            optimizer = AdamW(model.parameters(), lr=float(lr), weight_decay=float(weight_decay))\n            criterion = torch.nn.BCEWithLogitsLoss()\n            model.to(device)\n            model.train()\n            for epoch in range(num_epochs):\n                total_loss = 0\n                for batch in train_dataloader:\n                    batch = tuple(t.to(device) for t in batch)\n                    inputs = {'input_ids': batch[0],'attention_mask':batch[1],'labels':batch[2]}\n                    optimizer.zero_grad()\n                    outputs = model(**inputs)\n                    loss = outputs.loss\n                    loss.backward()\n                    optimizer.step()\n                    total_loss += loss.item()\n                print(weight_decay)\n                avg_loss = total_loss / len(train_dataloader)\n                evaluate_model_from_file(model,tokenizer, device, epoch, numLabels, lr)\n\n    # Asegúrate de que el modelo esté en modo de evaluación antes de guardarlo\n    #model.eval()\n\n    # Guardar el modelo entrenado\n    #ruta_modelo_guardado = '/kaggle/working/modelo-topics-v3-5epochs-400prueba2.pth'  # Especifica la ruta donde deseas guardar el modelo\n    # Guarda el modelo en el archivo especificado\n    #torch.save(model.state_dict(), ruta_modelo_guardado)\"\"\"","metadata":{"id":"At5zrGvqQ0IQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Evaluar el modelo en el conjunto de prueba\nval_loss = 0.0\nval_preds = []\ncriterion = torch.nn.CrossEntropyLoss()\n\nfor batch in val_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    inputs = {'input_ids': batch[0],'attention_mask':batch[1],'labels':batch[2]}\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n    val_loss += criterion(logits, batch[2]).item()  # Las etiquetas no deben ser convertidas a 'Float'\n    val_preds.extend(logits.argmax(dim=1).cpu().numpy())  # Obtenemos las predicciones de clases\n\n\n# Calcular métricas de evaluación\nval_preds = torch.tensor(val_preds)\nclassification_metrics = classification_report(val_labels, val_preds, target_names=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"])\nprint(classification_metrics)\nconf_matrix = confusion_matrix(val_labels, val_preds)\nprint(conf_matrix)'''\n","metadata":{"id":"TXuooz7CQ0IQ","trusted":true},"execution_count":null,"outputs":[]}]}